****************************************************************************************************************
Sqoop - Best Practices
****************************************************************************************************************

Sqoop uses MapReduce to import and export the data, which provides parallel operation as well as fault tolerance..

Tune the following Sqoop arguments in JDBC connection or Sqoop mapping to optimize performance

1. batch
2. split-by and boundary-query
3. direct
4. fetch-size
5. num-mapper

****************************************************************************************************************
Data in Batches
****************************************************************************************************************
Specifies that you can group the related SQL statements into a batch when you export data.

The JDBC interface exposes an API for doing batches in a prepared statement with multiple sets of values. 
With the --batch parameter, Sqoop can take advantage of this. 

Enable JDBC batching using the --batch parameter.

sqoop export  --connect <<JDBC URL>>  
              --username <<SQOOP_USER_NAME>>  
              --password <<SQOOP_PASSWOR>>   
              --table <<TABLE_NAME>>  
              --export-dir <<FOLDER_URI>>  
              --batch
              
The second option is to use the property sqoop.export.records.per.statement to specify the number of records
that will be used in each insert statement:

sqoop export  -Dsqoop.export.records.per.statement=10000
              --connect <<JDBC URL>>   
              --username <<SQOOP_USER_NAME>>   
              --password <<SQOOP_PASSWORD>>   
              --table <<TABLE_NAME>>   
              --export-dir <<FOLDER_URI>> 

You can set how many rows will be inserted per transaction with the 
sqoop.export.statements.per.transaction property:

sqoop export   -Dsqoop.export.statements.per.transaction=10000   
              --connect <<JDBC URL>>   
              --username <<SQOOP_USER_NAME>>   
              --password <<SQOOP_PASSWORD>>  
              --table <<TABLE_NAME>>  
              --export-dir <<FOLDER_URI>> 


****************************************************************************************************************
Custom Boundary Queries
****************************************************************************************************************

You can use boundary-query if you do not get the desired results by using the split-by argument alone.
Specify range of values that you can import. 

When you configure the boundary-query argument, you must specify the min(id) and max(id) along with the table name. 


sqoop import   --connect <<JDBC URL>>   
               --username <<SQOOP_USER_NAME>>   
               --password <<SQOOP_PASSWORD>>   
               --query <<QUERY>>
               --split-by <<ID>>
               --target-dir <<TARGET_DIR_URI>>
               --boundary-query "select min(<<ID>>), max(<<ID>>) from <<TABLE>>"



****************************************************************************************************************
Data Directly into Hive
****************************************************************************************************************

Specify direct import fast path when you import data from RDBMS.

Rather than using the JDBC interface for transferring data, the direct mode delegates the job of transferring data to the 
native utilities provided by the database vendor. In the case of MySQL, the mysqldump and mysqlimport will be used for 
retrieving data from the database server or moving data back. 


sqoop import   --connect <<JDBC URL>>   
               --username <<SQOOP_USER_NAME>>   
               --password <<SQOOP_PASSWORD>>  
               --table <<TABLE_NAME>>   
               --direct
  
  
****************************************************************************************************************               
Data using Fetch-size
****************************************************************************************************************

Specifies the number of entries that Sqoop can import at a time.

Use the following syntax:

--fetch-size=<n>

Where <n> represents the number of entries that Sqoop must fetch at a time. Default is 1000.

Increase the value of the fetch-size argument based on the volume of data that need to read. 
Set the value based on the available memory and bandwidth.



****************************************************************************************************************
Controlling Parallelism
****************************************************************************************************************

Specify number of map tasks that can run in parallel. Default is 4. To optimize performance, set the number of map tasks to a 
value lower than the maximum number of connections that the database supports.

Use the parameter --num-mappers or -m if you want Sqoop to use a different number of mappers. For example, to suggest 10 concurrent tasks, 
use the following Sqoop command:

sqoop import   --connect jdbc:mysql://mysql.example.com/sqoop   
               --username sqoop   
               --password sqoop   
               --table customers   
               --num-mappers 10

Controlling the amount of parallelism that Sqoop will use to transfer data is the main way to control the load on your database. 
Using more mappers will lead to a higher number of concurrent data transfer tasks, which can result in faster job completion. 
However, it will also increase the load on the database as Sqoop will execute more concurrent queries.


****************************************************************************************************************
Split-By
****************************************************************************************************************

Specifies the column name based on which Sqoop must split the work units.
If you do not specify a column name, Sqoop splits the work units based on the primary key.

Use the following syntax:
--split-by <column name>

sqoop import --connect <<JDBC URL>> 
             --username <<SQOOP_USER_NAME>> 
             --password <<SQOOP_PASSWORD>> 
             --query <<QUERY>> 
             --split-by <<ID>>
             
===================================================================================================================================
===================================================================================================================================

